{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53316b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from dataset import causal_mask\n",
    "from model import build_transformer\n",
    "from config import get_config, get_weights_file_path\n",
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).fill_(next_word.item()).type_as(source).to(device)],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        if next_word.item() == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def beam_search_decode(model, source, source_mask, tokenizer_tgt, max_len, device, beam_size=3):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    sequences = [[torch.tensor([sos_idx], device=device), 0.0]]  # (tokens, score)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            if seq[-1].item() == eos_idx:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            decoder_input = seq.unsqueeze(0)  # shape (1, current_seq_len)\n",
    "            decoder_mask = causal_mask(decoder_input.size(1)).to(device)\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "            logits = model.project(out[:, -1])  # (1, vocab_size)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_size, dim=-1)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                next_token = topk_indices[0, i].item()\n",
    "                next_score = score + topk_log_probs[0, i].item()\n",
    "                new_seq = torch.cat([seq, torch.tensor([next_token], device=device)])\n",
    "                all_candidates.append((new_seq, next_score))\n",
    "\n",
    "        # select best beam_size sequences\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "        sequences = ordered[:beam_size]\n",
    "\n",
    "        # if all candidates ended with EOS\n",
    "        if all(seq[-1].item() == eos_idx for seq, _ in sequences):\n",
    "            break\n",
    "\n",
    "    # Return the best sequence (excluding SOS)\n",
    "    best_seq = sequences[0][0]\n",
    "    return best_seq[1:] if best_seq[0].item() == sos_idx else best_seq\n",
    "\n",
    "\n",
    "def load_tokenizer(path):\n",
    "    return Tokenizer.from_file(path)\n",
    "\n",
    "def translate_sentence(model, sentence, tokenizer_src, tokenizer_tgt, config, device):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer_src.encode(sentence).ids\n",
    "    tokens = [tokenizer_src.token_to_id('[SOS]')] + tokens + [tokenizer_src.token_to_id('[EOS]')]\n",
    "\n",
    "    if len(tokens) < config['seq_len']:\n",
    "        tokens += [tokenizer_src.token_to_id('[PAD]')] * (config['seq_len'] - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:config['seq_len']]\n",
    "\n",
    "    encoder_input = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    encoder_mask = (encoder_input != tokenizer_src.token_to_id('[PAD]')).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# ============\n",
    "    source = tokenizer_src.encode(sentence).ids\n",
    "    source = torch.tensor(source, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(1).unsqueeze(2)\n",
    "    output_tokens = beam_search_decode(model, source, source_mask, tokenizer_tgt, config['seq_len'], device)\n",
    "\n",
    "# ===============\n",
    "\n",
    "    # output_tokens = greedy_decode(model, encoder_input, encoder_mask, tokenizer_tgt, config['seq_len'], device)\n",
    "    output_text = tokenizer_tgt.decode(output_tokens.tolist(), skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "## Use this code to load the state dict in case of using torch.compile() when compiling the model\n",
    "# as during model compilation it changes some paramters name\n",
    "\n",
    "def clean_state_dict(state_dict, prefix_to_strip=\"_orig_mod.\"):\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(prefix_to_strip):\n",
    "            new_k = k[len(prefix_to_strip):]  # Strip prefix\n",
    "        else:\n",
    "            new_k = k\n",
    "        new_state_dict[new_k] = v\n",
    "    return new_state_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32192e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!set CUDA_LAUNCH_BLOCKING=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2eb3a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 50,\n",
       " 'num_epochs': 100,\n",
       " 'lr': 0.001,\n",
       " 'seq_len': 32,\n",
       " 'd_model': 128,\n",
       " 'N': 2,\n",
       " 'd_ff': 256,\n",
       " 'head': 4,\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'hi',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': None,\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel',\n",
       " 'save_every': 20,\n",
       " 'warmup_steps': 4000,\n",
       " 'weight_decay': 0.01}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4eeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80175601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [201, 22, 16, 57]\n",
      "Decoded: देने एक . लोग\n"
     ]
    }
   ],
   "source": [
    "tokenizer_src = load_tokenizer(config['tokenizer_file'].format(config['lang_src']))\n",
    "tokenizer_tgt = load_tokenizer(config['tokenizer_file'].format(config['lang_tgt']))\n",
    "\n",
    "text = \"how are you ?\"\n",
    "tokens = tokenizer_src.encode(text).ids\n",
    "decoded = tokenizer_tgt.decode(tokens)\n",
    "print(\"Encoded:\", tokens)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8a74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Model and tokenizers loaded. Ready for inference.\n",
      "\n",
      "🌐 English Sentence: what is this ?\n",
      "\n",
      "🌐 Hindi Translation: क्या ये ? है ?\n",
      "\n",
      "================================\n",
      "🌐 English Sentence: who are you ?\n",
      "\n",
      "🌐 Hindi Translation: आप कौन हैं ?\n",
      "\n",
      "================================\n",
      "🌐 English Sentence: The court has fixed a hearing for February 12\n",
      "\n",
      "🌐 Hindi Translation: अदालत के लिए इस मामले में आगे की सुनवाई के लिए एक फरवरी की सुनवाई के लिए\n",
      "\n",
      "================================\n",
      "🌐 English Sentence: God has no figure, but because of allure He can be seen in various forms of God\n",
      "\n",
      "🌐 Hindi Translation: ईश्वर है , पर वो हमें ऐसे ही हैं क्योंकि ईश्वर में कई रूप से माना जाता है ।\n",
      "\n",
      "================================\n",
      "🌐 English Sentence: this is so interesting\n",
      "\n",
      "🌐 Hindi Translation: कि इस बात को दिलचस्प बात है कि\n",
      "\n",
      "================================\n",
      "🌐 English Sentence: what are you saying\n",
      "\n",
      "🌐 Hindi Translation: आप ये कह रहे हैं कि\n",
      "\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# Load Tokenizers\n",
    "tokenizer_src = load_tokenizer(config['tokenizer_file'].format(config['lang_src']))\n",
    "tokenizer_tgt = load_tokenizer(config['tokenizer_file'].format(config['lang_tgt']))\n",
    "\n",
    "# Load model\n",
    "model = build_transformer(\n",
    "    tokenizer_src.get_vocab_size(),\n",
    "    tokenizer_tgt.get_vocab_size(),\n",
    "    config['seq_len'], \n",
    "                              config['seq_len'], config['d_model'],config['N'],\n",
    "                              config['head'],0.1,config['d_ff']\n",
    ").to(device)\n",
    "\n",
    "model_path = get_weights_file_path(config, config['preload'])  # e.g., \"weights/04.pth\"\n",
    "model_path = r\"weights/tmodel_100.pt\"\n",
    "state = torch.load(model_path, map_location=device)\n",
    "\n",
    "cleaned_state_dict = clean_state_dict(state[\"model_state_dict\"])  # or whatever your key is\n",
    "model.load_state_dict(cleaned_state_dict)\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "print(\"🔥 Model and tokenizers loaded. Ready for inference.\\n\")\n",
    "\n",
    "while True:\n",
    "    english_input = input(\"Enter English sentence (or type 'exit' to quit): \").strip()\n",
    "    if english_input.lower() == \"exit\":\n",
    "        break\n",
    "    # Tokenize input\n",
    "    \n",
    "    hindi_output = translate_sentence(model, english_input, tokenizer_src, tokenizer_tgt, config, device)\n",
    "    print(f\"🌐 English Sentence: {english_input}\\n\")\n",
    "    print(f\"🌐 Hindi Translation: {hindi_output}\\n\")\n",
    "    print(\"=\"*32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33665a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE : God has no figure, but because of allure He can be seen in various forms of God\n",
    "TARGET : वैसे तो ईश्वर रूपहीन है पर माया की वजह से वो हमें कई देवताओं के रूप में प्रतीत हो सकता है।\n",
    "PREDICTED : ईश्वर को नहीं है क्योंकि वह सभी हिन्दू धर्म के कई देखा जाए ।"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
